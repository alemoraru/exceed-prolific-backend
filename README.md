# EXCEED Prolific Backend

This repository contains the backend code for the EXCEED Prolific application, a study aimed at investigating code
understanding and error correction using Python code snippets and (automatically rephrased) error messages. The backend
is built with FastAPI and SQLAlchemy, and provides RESTful APIs for participant management, code fix submissions, error
message feedback, and event logging.

---

## üß© Stack Overview

- **Python 3.12**
- **FastAPI** for API endpoints
- **SQLAlchemy** for ORM and database management
- **PostgreSQL** for database storage
- **Docker** for containerization
- **pytest** for testing API endpoints and functionality
- **unittest** for testing (i.e., testing participant submitted code snippets)
- **Ollama/ChatGPT** clients for LLM rephrasing of error messages

---

## üèóÔ∏è API Architecture

- Modular FastAPI routers for participants, code fix submissions, error message feedback, and events
- SQLAlchemy models for `participants`, `code_submissions`, `errors`, and `events`
- Evaluator service for syntax and semantic code checks
- LLM-based error rephrasing for educational feedback
- Data folder for code snippets, test suites, and error messages

---

## ‚ö° QuickStart

1. **Clone the repository:**
   ```bash
   git clone https://github.com/amoraru/exceed-prolific-backend.git
   cd exceed-prolific-backend
   ```
2. **Create a virtual environment in the root directory (optional but recommended):**
   ```bash
   python -m venv venv
   source venv/bin/activate
   ```
3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```
4. **Run the application:**
   ```bash
   uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
   ```
5. **Access the API docs (Swagger Documentation):**
   Visit [http://localhost:8000/docs](http://localhost:8000/docs)

> Note: Without any explicit environment variables set, the application will default to using the ones defined in the
`.env` file. If you want to use a different database or LLM model, make sure to set the appropriate environment
> variables before running the application. Check the next section for the list of environment variables.

---

## ‚öôÔ∏è Environment Variables

| Variable         | Description                                                              | Example Value                                                                                           | Required |
|------------------|--------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|----------|
| `DATABASE_URL`   | PostgreSQL connection string                                             | `postgresql://admin:admin@localhost:5432/prolific`                                                      | yes      |
| `OLLAMA_URL`     | URL for the Ollama LLM service                                           | http://localhost:11434                                                                                  | yes      |
| `OLLAMA_MODEL`   | Ollama model to use for error rephrasing                                 | `llama3.1:8b` (default value)                                                                           | yes      |
| `FRONTEND_URL`   | Allowed frontend origin for CORS                                         | http://localhost:3000                                                                                   | no       |
| `OPENAI_API_KEY` | API key for OpenAI (used for LLM error rephrasing if ChatGPT is enabled) | your_openai_api_key_here -> note that we do not use the ChatGPT Client unless modifying the actual code | no       |      

> **Note**: The `OLLAMA_MODEL` variable is set to `llama3.1:8b` by default, which is the model that we have used
> for rephrasing error messages. If you want to use a different model, make sure to set the `OLLAMA_MODEL`
> environment variable to the desired model name. However, this will work only if you actually have the model
> downloaded on your machine using Ollama. The full list of available models set for the backend can be found within
> the `app/services/llm/llm_client.py` and `app/utils/enums.py` files.

### OLLAMA_MODEL

While our application allows technically for any Ollama model to be used, we recommend using one of the following
models:

- [`llama3.1:8b`](https://ollama.com/library/llama3.1:8b)
- [`qwen2.5:7b`](https://ollama.com/library/qwen2.5:7b)
- [`qwen2.5-coder:7b`](https://ollama.com/library/qwen2.5-coder:7b)
- [`granite3.3:8b`](https://ollama.com/library/granite3.3:8b)

---

## üìö API Endpoints

Listing of available API endpoints would be too long for this README, therefore we recommend checking out the
[API documentation](http://localhost:8000/docs) for a complete overview of the available endpoints and their
descriptions. The API documentation is automatically generated by FastAPI and provides detailed information
about each endpoint, including request and response schemas, query parameters, and example requests.

This can be accessed by running the application and navigating to the `/docs` endpoint in your browser.

---

## üìù Notes

- The backend is naturally designed for integration with a frontend survey application (the implementation for
  that is located in a separate GitHub repository:
  [EXCEED Prolific Frontend](https://github.com/alemoraru/exceed-prolific-frontend)).

---

## üõ†Ô∏è Prerequisites

- [Python 3.12+](https://www.python.org/downloads/)
- [Docker](https://www.docker.com/get-started) (for containerized deployment only, optional)
- [PostgreSQL](https://www.postgresql.org/download/) (needed for database management - we recommend using Docker for
  this)
    - If using Docker, you can make use of the provided `docker-compose.yml` file to set up a PostgreSQL database:
      ```bash
      docker-compose up -d
      ```
- [Ollama](https://ollama.com/) (for LLM rephrasing of error messages - optional, but required for the second
  attempt at fixing code snippets).
    - Make sure that you have downloaded the model you want to use within the code.

---

## ü§ù Contributing

This project was developed as part of the EXCEED MSc Thesis project at Technische Universiteit Delft. As such,
contributions of any sort will not be accepted. This repository is provided for replication and educational purposes
ONLY. Since it was used to orchestrate the deployment of our study on Prolific, it is NOT intended for further
development or contributions.

---

## üìÑ License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
